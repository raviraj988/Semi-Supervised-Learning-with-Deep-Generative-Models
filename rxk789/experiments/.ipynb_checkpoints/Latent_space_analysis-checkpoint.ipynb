{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d13b811-bd78-45e4-b017-d7f25d659648",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets, transforms\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "from torch.optim import Adam\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Set up project paths\n",
    "def setup_paths():\n",
    "    cwd = os.getcwd()\n",
    "    project_root = os.path.abspath(os.path.join(cwd, \"..\"))\n",
    "    src_path = os.path.join(project_root, \"src\")\n",
    "    if src_path not in sys.path:\n",
    "        sys.path.append(src_path)\n",
    "\n",
    "setup_paths()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f629c287-4ddf-4a04-be43-2479b45ec0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import VAE models and utilities\n",
    "from model import MNISTVariationalAutoEncoder, CIFAR10VariationalAutoEncoder\n",
    "from utils import m2_loss_labeled, validate\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1eab4826-1731-4642-a620-d6bf2573b694",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function to load dataset\n",
    "def load_dataset(dataset_name):\n",
    "    if dataset_name == \"MNIST\":\n",
    "        transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5,), (0.5,))\n",
    "        ])\n",
    "        dataset = datasets.MNIST(root=\"../data\", train=True, transform=transform, download=True)\n",
    "        test_dataset = datasets.MNIST(root=\"../data\", train=False, transform=transform, download=True)\n",
    "        model = MNISTVariationalAutoEncoder(latent_dim=128, num_classes=10).to(device)\n",
    "    else:\n",
    "        transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "        ])\n",
    "        dataset = datasets.CIFAR10(root=\"../data\", train=True, transform=transform, download=True)\n",
    "        test_dataset = datasets.CIFAR10(root=\"../data\", train=False, transform=transform, download=True)\n",
    "        model = CIFAR10VariationalAutoEncoder(latent_dim=128, num_classes=10).to(device)\n",
    "\n",
    "    return dataset, test_dataset, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96c3d6d7-d7cc-4b37-9c1e-69b2eadb54d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train the model on labeled data only\n",
    "def train_labeled(model, loader, optimizer, criterion, num_epochs=15):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        for x, y in loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Labeled data loss\n",
    "            y_onehot = F.one_hot(y, model.num_classes).float().to(device)\n",
    "            recon, mean, log_var, logits = model(x, y_onehot=y_onehot)\n",
    "            loss = criterion(recon, x, mean, log_var, logits, y)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f\"Labeled Epoch {epoch + 1}/{num_epochs}, Loss: {total_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b3e38eee-338c-46c5-8ddc-43e024983a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train the model on the full dataset (labeled + unlabeled)\n",
    "def train_full(model, labeled_loader, unlabeled_loader, optimizer, criterion_labeled, criterion_unlabeled, num_epochs=15):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        for (x_labeled, y_labeled), (x_unlabeled, _) in zip(labeled_loader, unlabeled_loader):\n",
    "            x_labeled, y_labeled = x_labeled.to(device), y_labeled.to(device)\n",
    "            x_unlabeled = x_unlabeled.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Labeled data loss\n",
    "            y_onehot = F.one_hot(y_labeled, model.num_classes).float().to(device)\n",
    "            recon_labeled, mean_l, log_var_l, logits_l = model(x_labeled, y_onehot=y_onehot)\n",
    "            loss_labeled = criterion_labeled(recon_labeled, x_labeled, mean_l, log_var_l, logits_l, y_labeled)\n",
    "\n",
    "            # Unlabeled data loss\n",
    "            recon_unlabeled, mean_u, log_var_u, logits_u = model(x_unlabeled)\n",
    "            loss_unlabeled = criterion_unlabeled(recon_unlabeled, x_unlabeled, mean_u, log_var_u, logits_u)\n",
    "\n",
    "            # Combine losses\n",
    "            loss = loss_labeled + loss_unlabeled\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f\"Full Dataset Epoch {epoch + 1}/{num_epochs}, Loss: {total_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b3ec6788-e592-4628-ad46-9ee2dd87b9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to save the trained model\n",
    "def save_model(model, path):\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    torch.save(model.state_dict(), path)\n",
    "    print(f\"Model saved to: {path}\")\n",
    "\n",
    "# Function to load the trained model\n",
    "def load_model(model, path):\n",
    "    if os.path.exists(path):\n",
    "        model.load_state_dict(torch.load(path, map_location=device))\n",
    "        print(f\"Model loaded successfully from {path}\")\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"Checkpoint not found at {path}. Ensure the model is trained and saved.\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "72d0ec21-75f8-4338-a210-020e9d1c2970",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract logits and images\n",
    "def extract_logits_and_images(model, data_loader):\n",
    "    model.eval()\n",
    "    logits_list, labels_list, images_list = [], [], []\n",
    "    with torch.no_grad():\n",
    "        for x, y in data_loader:\n",
    "            x = x.to(device)\n",
    "            _, _, _, logits = model(x)\n",
    "            logits_list.append(logits.cpu().numpy())\n",
    "            labels_list.append(y.numpy())\n",
    "            images_list.append(x.cpu().numpy())\n",
    "    return np.concatenate(logits_list), np.concatenate(labels_list), np.concatenate(images_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7bdf9a0c-a5a3-456f-8e74-0168e708501a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization functions\n",
    "def plot_logits_and_images(images, logits, labels, dataset_name, num_samples=5):\n",
    "    softmax_logits = torch.softmax(torch.tensor(logits), dim=1).numpy()\n",
    "    sample_indices = np.random.choice(len(logits), num_samples, replace=False)\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    for i, idx in enumerate(sample_indices):\n",
    "        plt.subplot(2, num_samples, i + 1)\n",
    "        img = images[idx]\n",
    "        if dataset_name == \"MNIST\":\n",
    "            plt.imshow(img.squeeze(), cmap=\"gray\")\n",
    "        else:\n",
    "            img = np.transpose(img, (1, 2, 0))\n",
    "            plt.imshow((img - img.min()) / (img.max() - img.min()))\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(f\"Label: {labels[idx]}\")\n",
    "        plt.subplot(2, num_samples, num_samples + i + 1)\n",
    "        plt.bar(range(len(softmax_logits[idx])), softmax_logits[idx])\n",
    "        plt.title(\"Softmax of Logits\")\n",
    "        plt.xlabel(\"Class\")\n",
    "        plt.ylabel(\"Probability\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6f1db19e-5798-4260-a0f4-de5d87ee64d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_latent_space(logits, labels, method=\"PCA\"):\n",
    "    if method == \"PCA\":\n",
    "        reducer = PCA(n_components=2)\n",
    "    elif method == \"TSNE\":\n",
    "        reducer = TSNE(n_components=2, random_state=42)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid method. Choose 'PCA' or 'TSNE'.\")\n",
    "    reduced_latents = reducer.fit_transform(logits)\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    scatter = plt.scatter(reduced_latents[:, 0], reduced_latents[:, 1], c=labels, cmap='tab10', alpha=0.7)\n",
    "    plt.colorbar(scatter, label='Class Label')\n",
    "    plt.title(f\"Latent Space Visualization ({method})\")\n",
    "    plt.xlabel(f\"{method} Component 1\")\n",
    "    plt.ylabel(f\"{method} Component 2\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d489f4d5-976c-47a6-919a-bb5b39677202",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m dataset_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMNIST\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Change to \"CIFAR10\" for CIFAR-10\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m dataset, test_dataset, vae_model \u001b[38;5;241m=\u001b[39m load_dataset(dataset_name)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Split dataset for labeled and unlabeled training\u001b[39;00m\n\u001b[1;32m      5\u001b[0m labeled_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(dataset) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m0.1\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 3)"
     ]
    }
   ],
   "source": [
    "dataset_name = \"MNIST\"  # Change to \"CIFAR10\" for CIFAR-10\n",
    "dataset, test_dataset, vae_model = load_dataset(dataset_name)\n",
    "\n",
    "# Split dataset for labeled and unlabeled training\n",
    "labeled_size = int(len(dataset) * 0.1)\n",
    "unlabeled_size = len(dataset) - labeled_size\n",
    "labeled_data, unlabeled_data = random_split(dataset, [labeled_size, unlabeled_size])\n",
    "labeled_loader = DataLoader(labeled_data, batch_size=128, shuffle=True)\n",
    "unlabeled_loader = DataLoader(unlabeled_data, batch_size=128, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=256, shuffle=False)\n",
    "\n",
    "# Train on labeled data only\n",
    "optimizer = Adam(vae_model.parameters(), lr=1e-3)\n",
    "train_labeled(vae_model, labeled_loader, optimizer, m2_loss_labeled, num_epochs=15)\n",
    "save_model(vae_model, \"../trained_models/vae_labeled_only.pth\")\n",
    "\n",
    "# Train on the full dataset (labeled + unlabeled)\n",
    "vae_model.load_state_dict(torch.load(\"../trained_models/vae_labeled_only.pth\", map_location=device))\n",
    "train_full(vae_model, labeled_loader, unlabeled_loader, optimizer, m2_loss_labeled, m2_loss_unlabeled, num_epochs=15)\n",
    "save_model(vae_model, \"../trained_models/vae_full_trained.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3addf741-52d8-4446-a95f-7c78bd78713e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model and analyze logits\n",
    "vae_model = load_model(vae_model, \"../trained_models/vae_labeled_only.pth\")\n",
    "logits, labels, images = extract_logits_and_images(vae_model, test_loader)\n",
    "plot_logits_and_images(images, logits, labels, dataset_name)\n",
    "visualize_latent_space(logits, labels, method=\"PCA\")\n",
    "visualize_latent_space(logits, labels, method=\"TSNE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95d5402-13c3-4dd6-bea1-94982327bd89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model and analyze logits\n",
    "vae_model = load_model(vae_model, \"../trained_models//vae_full_trained.pth\")\n",
    "logits, labels, images = extract_logits_and_images(vae_model, test_loader)\n",
    "plot_logits_and_images(images, logits, labels, dataset_name)\n",
    "visualize_latent_space(logits, labels, method=\"PCA\")\n",
    "visualize_latent_space(logits, labels, method=\"TSNE\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
